{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Decision Trees\n",
    "\n",
    "In this assignment you will implement a Decision Tree algorithm as learned in class.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "2. Write vectorized code whenever possible.\n",
    "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Write your functions in the provided `hw2.py` python module only. All the logic you write is imported and used in this jupyter notebook.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. Any other imports detected in `hw2.py` will earn you the grade of 0, even if you only used them for testing.\n",
    "6. Your code must run without errors. During the environment setup, you were given a specific version of `numpy` to install. Changes of the configuration we provided are at your own risk. Code that cannot run will also earn you the grade of 0.\n",
    "7. Write your own code. Cheating will not be tolerated. \n",
    "8. Submission includes the `hw2.py` file and this notebook. Answers to qualitative questions should be written in markdown cells (with $\\LaTeX$ support).\n",
    "9. You are allowed to include additional functions.\n",
    "10. Submission: zip only the completed jupyter notebook and the python file `hw2.py`.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Practice OOP in python.\n",
    "2. Implement two impurity measures: Gini and Entropy.\n",
    "3. Implement a decision tree from scratch.\n",
    "4. Prune the tree to achieve better results.\n",
    "5. Visualize your results and the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from decision_trees import * # this imports all functions from hw2.\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing\n",
    "\n",
    "We will use the breast cancer dataset that is available as a part of sklearn - a popular machine learning and data science library in python. In this example, our dataset will be a single matrix with the **labels on the last column**. Notice that you are not allowed to use additional functions from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape:  (426, 31)\n",
      "Testing dataset shape:  (143, 31)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load dataset\n",
    "X, y = datasets.load_breast_cancer(return_X_y = True)\n",
    "X = np.column_stack([X,y]) # the last column holds the labels\n",
    "\n",
    "# split dataset\n",
    "X_train, X_test = train_test_split(X, random_state=99)\n",
    "\n",
    "print(\"Training dataset shape: \", X_train.shape)\n",
    "print(\"Testing dataset shape: \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity Measures\n",
    "\n",
    "Implement the functions `calc_gini` and `calc_entropy` in the python file `hw2.py`. You are encouraged to test your implementation using the cell below. We'll denote S as the dataset and $\\phi$ as the impurity measure.\n",
    "\n",
    "The Gini Impurity can be calculated by \n",
    "$$\\phi(S) = 1 - \\sum_{i=1}^{c}(p_i)^2 = 1 - \\sum_{i=1}^{c} (\\frac{|S_i|}{|S|})^2$$\n",
    "\n",
    "and the Shannon Entropy can be calculated by\n",
    "$$\\phi(S) = -\\sum_{i=1}^{c}p_i\\log_2(p_i) = -\\sum_{i=1}^{c}\\frac{|S_i|}{|S|}\\log_2(\\frac{|S_i|}{|S|})$$\n",
    "\n",
    "where $|S_i|$ is the size of class $i$ in the dataset $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓\n"
     ]
    }
   ],
   "source": [
    "equally_distributed_data = np.array([[1],[0],[1],[0]])\n",
    "gini_impurity = calc_gini(equally_distributed_data)\n",
    "print('✓' if np.equal(gini_impurity, 0.5) else '✗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓\n"
     ]
    }
   ],
   "source": [
    "shannon_entropy = calc_entropy(equally_distributed_data)\n",
    "print('✓' if np.equal(shannon_entropy, 1) else '✗')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data\n",
    "\n",
    "Before we start building the decision tree we need a few metrics for splitting the data.\n",
    "\n",
    "1. Since the data consists of continuous values. We will split the dataset according to a certain threshold. The dataset will be split into two according to every row where the specific feature is smaller than the threshold and larger than the threshold.\n",
    "    - The thresholds will be calculated as the average of every consecutive pair in a vector. For example if the vector looks like [1,2,3,4,5] then the thresholds will be [1.5, 2.5, 3.5, 4.5]. \n",
    "2. Once we have all the thresholds for a certain feature we need to find the threshold that splits the data the best.\n",
    "    - The best threshold is the one that maximizes the equation\n",
    "    \n",
    "    $$\n",
    "    \\phi(S,A) = \\phi(S) - \\sum_{v \\in Values(A)}\\frac{|S_v|} {|S|} \\phi(S_v)\n",
    "    $$\n",
    "    \n",
    "        where S is the dataset, A is the feature to split by, and Values(A) are the different classes of the dataset\n",
    "        which is 0 and 1 in our case.\n",
    "    - We choose the feature we want to split by, split the data according to each possible threshold and keep the    threshold with the best information .gain\n",
    "3. Once we are able to find the best threshold for each feature, we need to find the best feature to split the data by. \n",
    "    - The best feature is the one that maximizes the same equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  1],\n",
       "       [12,  1],\n",
       "       [ 0,  0],\n",
       "       [15,  1],\n",
       "       [ 6,  0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test for best_threshold\n",
    "A = np.array([7,12,0,15,6])\n",
    "A_labels = np.array([1,1,0,1,0])\n",
    "A = np.column_stack((A, A_labels))\n",
    "A\n",
    "# Notice every row where the 0th feature is < 6.5 has a label of 0\n",
    "# and every row where it's > 6.5 has a label of 1.\n",
    "# So the threshold 6.5 splits the data perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll find the best threshold for the only feature\n",
    "best_threshold(A, 0, calc_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6, 19, 14,  7,  1],\n",
       "       [10,  7,  6, 12,  1],\n",
       "       [18, 10, 10,  0,  0],\n",
       "       [ 3,  7,  2, 15,  1],\n",
       "       [ 1, 11,  5,  6,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we'll add more features to A and compute the best feature to split it by\n",
    "A = np.column_stack((np.random.randint(0, 20, (5,3)), A))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best feature is feature #3. The best associated threshold is 6.5\n"
     ]
    }
   ],
   "source": [
    "# Note the feature index is 0 relative. Look at the output and convince yourself that it makes sense.\n",
    "feature, threshold = best_feature_threshold(A, calc_gini)\n",
    "print(f\"The best feature is feature #{feature}. The best associated threshold is {threshold}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree\n",
    "\n",
    "Use a Python class to construct the decision tree (look at the `DecisionNode` class in the python file `hw2.py`. Your class should support the following functionality:\n",
    "\n",
    "1. Initiating a node for a decision tree. You will need to use several class methods and class attributes and you are free to use them as you see fit. We recommend that every node will hold the feature and value used for the split and its children.\n",
    "2. Your code should support both Gini and Entropy as impurity measures. \n",
    "3. The provided data includes continuous data. In this exercise, create at most a single split for each node of the tree. The threshold you need to use for this exercise are the average of each consecutive pair of values. For example, assume some features contains the following values: [1,2,3,4,5]. You should use the following thresholds [1.5, 2.5, 3.5, 4.5]. \n",
    "4. When constructing the tree, test all possible thresholds for each feature. The stopping criteria is a pure tree.\n",
    "\n",
    "Complete the class `DecisionNode` in the python file `hw2.py`. The structure of this class is entirely up to you. Complete the function `build_tree` in the python file `hw2.py`. This function should get the training dataset and the impurity as inputs, initiate a root for the decision tree and construct the tree according to the procedure you learned in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.5 s, sys: 113 ms, total: 16.6 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# python support passing a function as arguments to another function.\n",
    "tree_gini = build_tree(data=X_train, impurity=calc_gini) \n",
    "tree_entropy = build_tree(data=X_train, impurity=calc_entropy, chi_value=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(tree_gini)\n",
    "print()\n",
    "for leaf in list_leaves(tree_gini, []):\n",
    "    print(leaf, leaf.parent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree evaluation\n",
    "\n",
    "Complete the functions `predict` and `calc_accuracy` in the python file `hw2.py`. You are allowed to implement this functionality as a class method.\n",
    "\n",
    "After building both trees using the training set (using Gini and Entropy as impurity measures), you should calculate the accuracy on the test set and print the measure that gave you the best test accuracy. For the rest of the exercise, use that impurity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test predict function ####\n",
    "instance = X_train[0]\n",
    "label = instance[-1]\n",
    "\n",
    "prediction = predict(tree_gini, instance)\n",
    "print('✓' if np.equal(prediction, label) else '✗')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_accuracy = calc_accuracy(tree_gini, X_test)\n",
    "entropy_accuracy = calc_accuracy(tree_entropy, X_test)\n",
    "\n",
    "print(f\"Gini accuracy = {gini_accuracy:.2f}\\nEntropy accuracy = {entropy_accuracy:.2f}\")\n",
    "\n",
    "best_tree = tree_gini if gini_accuracy > entropy_accuracy else tree_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi square pre-pruning\n",
    "\n",
    "Consider the following p-value cut-off values: [1 (no pruning), 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00001]. For each value, construct a tree and prune it according to the cut-off value. Next, calculate the training and testing accuracy. On a single plot, draw the training and testing accuracy as a function of the p-value. What p-value gives you the best results? Does the results support the theory you learned in class regarding Chi square pruning? Explain.\n",
    "\n",
    "**Note**: You need to change the `DecisionNode` to support Chi square pruning. Make sure the `chi_value=1` corresponds to no pruning. The values you need from the Chi square table are available in the python file `hw2.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "testing  = []\n",
    "\n",
    "p_values = [1, 0.01, 0.005, 0.001, 0.0005, 0.0001, 0.00001]\n",
    "#### Your code here ####\n",
    "for chi_value in p_values:\n",
    "    tree = build_tree(data=X_train, impurity=calc_entropy, chi_value=chi_value)\n",
    "    training.append(calc_accuracy(tree, X_train))\n",
    "    testing.append(calc_accuracy(tree, X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your visualization here ####\n",
    "plt.plot(p_values, training, label='training set', marker='o')\n",
    "plt.plot(p_values, testing, label='test set', marker='o')\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.xlim(1.1, .000008)\n",
    "plt.xlabel('chi_value')\n",
    "plt.ylabel('acurracy')\n",
    "plt.title('Acurracy as a function of chi_values')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that with pruning the accuracy of our training set declines but the accuracy of the test set actually increases up to chi = .005  before it starts to decline again. It increase the accuracy a tiny bit because we're reducing our chances of overfitting the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post pruning\n",
    "\n",
    "Construct a decision tree without Chi square pruning. For each leaf in the tree, calculate the test accuracy of the tree assuming no split occurred on the parent of that leaf and find the best such parent (in the sense that not splitting on that parent results in the best testing accuracy among possible parents). Make that parent into a leaf and repeat this process until you are left with just the root. On a single plot, draw the training and testing accuracy as a function of the number of internal nodes in the tree. Explain the results: what would happen to the training and testing accuracies when you remove nodes from the tree? Can you suggest a different approach to achieve better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print the tree\n",
    "\n",
    "Complete the function `print_tree` in the python file `hw2.py` and print the tree using the chosen impurity measure and no pruning. Your code should like something like this:\n",
    "```\n",
    "[X0 <= 1],\n",
    "  [X1 <= 2]\n",
    "    [X2 <= 3], \n",
    "       leaf: [{1.0: 10}]\n",
    "       leaf: [{0.0: 10}]\n",
    "    [X4 <= 5], \n",
    "       leaf: [{1.0: 5}]\n",
    "       leaf: [{0.0: 10}]\n",
    "   leaf: [{1.0: 50}]\n",
    "```\n",
    "\n",
    "where the number after X is the column index of the feature and the value is the threshold used to split the data\n",
    "of that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here ####\n",
    "print_tree(tree_gini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
